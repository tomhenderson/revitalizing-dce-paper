%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2022}
\acmYear{2022}
\setcopyright{rightsretained}
\acmConference[WNS3 2022]{Proceedings of the WNS3 2022}{June
22--23, 2022}{Virtual Event, USA}
\acmBooktitle{Proceedings of the WNS3 2022 (WNS3 2022), June 22--23,
2022, Virtual Event, USA}\acmDOI{10.1145/3532577.3532606}
\acmISBN{978-1-4503-9651-6/22/06}

\usepackage{listings}
%\newcommand{\subparagraph}{}
\lstdefinestyle{CStyle}{
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    language=C
}

\begin{document}
%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
%
\title{Revitalizing ns-3's Direct Code Execution}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
%
\author{Parth Pratim Chatterjee}
\affiliation{%
  \institution{Kalinga Institute of Industrial Technology}
  \city{Bhubaneswar}
  \state{Odisha}
  \country{India}
}
\email{parth27official@gmail.com}
\author{Thomas R. Henderson}
\affiliation{%
  \institution{University of Washington}
  \city{Seattle}
  \state{Washington}
  \country{USA}
}
\email{tomhend@uw.edu}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%\renewcommand{\shortauthors}{Chatterjee and Henderson}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
This paper reports on significant updates that were made to ns-3's Direct Code
Execution (DCE) feature to restore its compatibility with modern Linux
distributions and kernel stacks.  Recent versions of the GNU C standard
library introduced security features that blocked DCE's preferred mode of 
operation, and DCE's ability to incorporate the
TCP/IP kernel stack from recent Linux kernels was also disrupted by kernel
evolution.  This paper describes the challenges and possible solutions to
each issue and summarizes the solution that was implemented to restore
compatibility.  Finally, a Docker-based environment was configured to ease the
installation and usage of DCE for ns-3, expanding the range of Linux
distributions supported.  The result of this project is a forthcoming
DCE release supporting Ubuntu 20.04 distributions natively, and others
via Docker, and initial support for Linux kernel 5.10 use for IPv4-based
applications.  Our performance evaluation highlights the need to next work
on performance profiling to improve execution speed.
\end{abstract}

%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011006.10011066.10011070</concept_id>
       <concept_desc>Software and its engineering~Application specific development environments</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Application specific development environments}

\keywords{ns-3, network simulation}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{INTRODUCTION}

ns-3 is a popular discrete-event network simulator, written in C++, with
simulation models across all layers of the network stack.  The lowest level
of abstraction in ns-3 is typically the MAC-level frame, and for mainstream
protocols such as TCP/IP, ns-3 aims to be standards compliant in how it
encodes packets.  By default, ns-3 runs its simulations in virtual time,
as fast or as slow as the CPU can process the necessary simulation events.

One feature of ns-3 that differentiates it from other similar simulation tools
is its ability to use some real applications and network stacks directly
in a simulation context, using the same virtual clock.  This feature is
known as Direct Code Execution (DCE) \cite{Tazaki13}, \cite{Lacage10}.  In
brief, DCE enables selected (C or C++) applications to be compiled in 
a special way that allows them to be linked against DCE libraries that
provide POSIX functions, such as timekeeping and network sockets, that
are bound to ns-3 equivalents.  This allows ns-3 simulations to incorporate
selected applications directly rather than requiring dedicated ns-3 models.
Very little code in the application typically needs to be adapted to
be used in DCE; this helps to minimize long-term maintenance issues, as
newer versions of applications do not need intrusive changes to keep
working with ns-3.  DCE was later extended to cover the TCP/IP kernel
code for Linux and FreeBSD operating systems.  In short, DCE provides ns-3 users 
with the ability to run high-fidelity implementations of complicated
protocols (e.g., modern TCP, routing protocols such as OSPF and BGP)
without implementing them from scratch or making intrusive changes to
the original code. 

DCE is a sophisticated piece of software, and after an intensive period
of development and initial releases, the maintenance of DCE has been
semi-dormant for the past five years or so.  As a result, the host
operating systems that DCE is compatible with are stuck in the past.
The most recent Linux distribution compatible with the latest (DCE-1.11)
release is Ubuntu 16.04.  It has been estimated
 that, in 2020, the Linux kernel comprised 27.8 million lines of code and
experienced 75,000 commits in the past
year.\footnote{\url{https://www.theregister.com/2020/01/06/linux\_2020\_kernel\_systemd\_code}}
Although
DCE does not have to change for most of these commits, 
the host operating system environment that DCE both emulates and exists
within undergoes frequent change, leading to a DCE maintenance burden.

This paper reports on a recent ns-3 Google Summer of Code project to restore
DCE's compatibility with
recent Linux releases and kernel stack 
versions.\footnote{Final report and information on reproducing this paper's data can be found at \url{https://www.nsnam.org/wiki/GSOC2021DCE}} 
The project required
a lot of adjustments to DCE to account for changes in Linux and in the
C (and C++) standard library (on Linux, this library is called \emph{glibc}).
To simplify the installation burden on users, we also developed a Docker
container framework with a pre-built modified glibc.  
Finally, we updated the
version of the Linux kernel library for DCE from kernel version 4.4 
to kernel version 5.10.  A new DCE release (1.12) is being prepared with
capability to run on Ubuntu 20.04, and the use of Docker will facilitate
use on other modern Linux systems as well.

The rest of this paper is organized as follows.  Section 2 provides a summary
overview of DCE.  Sections 3 through 5 describe in detail the challenges
in upgrading DCE; Section 3 discusses core system issues, Section 4 discusses
network stack upgrades, and Section 5 discusses build and installation.
Section 6 presents a performance evaluation of the new software.
Section 7 describes related work.
Finally, Section 8 concludes with directions for future work.

\begin{figure*}[h!]
  \centering
    \includegraphics[width=0.9\textwidth]{figs/architecture.png}
  \caption{DCE Architecture (Redrawn from Figure 1 of [1]) with Recent Extensions Highlighted}
  \label{fig:architecture}
\end{figure*}

\section{DCE OVERVIEW}

ns-3 is designed as a set of simulation libraries that can be linked together
with a main program.  DCE extends ns-3 by providing several additional
libraries that enable the reuse of pre-existing application and network
stack code within ns-3.
Figure \ref{fig:architecture} provides an architectural overview of DCE;
this figure is partially redrawn from a similar Figure 1 of \cite{Tazaki13},
with extensions to highlight the contributions reported on herein.
In the lower center are ns-3 libraries; these libraries, along with simulation
programs that link to them, are the typical ns-3 simulation components.
Sitting above ns-3, in this diagram, is DCE.  DCE provides additional
libraries that can be used in place of ns-3 libraries, at the TCP/IP and
application layers.  The box bounding TCP, UDP, IPv4, and IPv6 depicts 
how the Linux (or FreeBSD) kernel is itself built as a library.  Data
packets flow across the boundary between a Linux \texttt{struct net\_device} and
the ns-3 NetDevice object, and additional system support, such as providing
time routines to kernel code, is synchronized with the ns-3 core.
The DCE virtualization core layer manages resources for the various DCE
applications, including file system access, heap- and stack-allocated
memory, global variable handling, and thread management.
Above the (optional) DCE kernel library is a POSIX interface layer that
DCE applications link to.  This layer redirects system calls, such as a
call to open a socket, to ns-3 or the DCE kernel; in essence, it reimplements
and redirects parts of the C standard library.  DCE applications are compiled
and linked with a few extra arguments such that they are built as 
position-independent executables (PIE), and a special DCE Executable and
Linkable Format (ELF) program loader
ensures that all of the application's symbols can be loaded multiple times
(as needed) into the ns-3 address space.

\section{CORE SYSTEM ISSUES}
The central capability of DCE is the provision of selected core system services
in a way that enables selected data flow to bypass normal system interfaces
and instead be redirected to or managed by the simulator.  This section
describes some challenges in maintaining techniques that DCE had been
relying on but that became unavailable in recent Linux versions.

\subsection{\emph{libio vtable} Redirection}
DCE user applications will typically write to output streams (e.g., stdout).
Since DCE may
load multiple application instances, printing to standard output
would give unordered logs and overwriting.
To solve this, DCE hijacks the FILE vtable to take control over FILE operations and then maps I/O to separate log files in organized log folders.

 \emph{libio} is the glibc component that handle streams.  
The \emph{vtable} is a table maintaining references to functions called for virtual functions defined for a class or an entity, for run-time dispatch of function calls. 
The FILE structure is a library defined structure which defines the overall organization, 
orientation and properties of any file I/O stream opened
by the host application. It maintains different parameters for storing useful operational fields like the UNIX based file descriptor number of the 
opened stream, the read/write offsets and buffer addresses of the stream. The pseudoname for the FILE structure as seen inside libc is \textit{\_IO\_FILE}. 
Since FILE is a library defined entity, the library provides its own set of implementations for all possible operations on an open FILE stream.
Whenever an \textit{\_IO\_FILE} stream is allocated by the kernel, a contiguous memory location is allocated as a block called \textit{\_IO\_FILE\_plus}. 
The \textit{\_IO\_FILE\_plus} structure is declared as follows:

\begin{lstlisting}[style=CStyle,numbers=none]
struct _IO_FILE_plus
{
  FILE file;
  const struct _IO_jump_t *vtable;
};
\end{lstlisting}

The implementation of the kernel's memory allocation processes ensures that the contents of a struct are allocated in contiguous memory locations. 
The FILE and the \textit{\_IO\_jump\_t} objects are allocated in contiguous memory locations. Specifically, the 
\textit{\_IO\_jump\_t} areas is interesting to DCE, as it defines the callbacks or reference pointers to the functions handlers for each supported file 
operations such as \texttt{read}, \texttt{close}, \texttt{seek}, \texttt{write} and \texttt{stat}.

This structure acts like the vtable for the FILE structure, but it does not behave like the ordinary vtable seen when working with virtual functions 
and derived classes, which are dynamic and supports run time bindings. This vtable is instead expected to behave as a statically bound vtable.  There exist 
other libc functions, like \textit{fopencookie}, to override some of the FILE operation implementations, but not all operations are supported, and 
\textit{fopencookie} also does not attach itself to a standard file I/O stream, 
instead works  with user defined buffers, otherwise known as cookies. 

Taking advantage of the contiguous memory allocation of the FILE and \textit{\_IO\_jump\_t}, one can overwrite the vtable with a custom vtable definition for all
the operations needed for DCE. One can make a dummy \textit{\_IO\_FILE\_plus} 
pointer point to the explicitly casted FILE object, \textit{memcpy} the existing vtable to a local copy, modify and overwrite the stream operations with
custom written function pointers, and then re-point the vtable field of the dummy \textit{\_IO\_FILE\_plus} to the local modified vtable.  This allows
control over stream operations, which can now be routed through DCE and made to behave as ns-3 streams, Unix FD streams, etc. based on the type of file 
descriptor that is defined.

\sloppy After libc version 2.25, security features have been implemented to identify 
and prevent block buffer overflow attacks on the FILE object. Whenever any FILE
operation is executed, glibc 
verifies the FILE object's vtable. It makes a call to 
\textit{\_IO\_validate\_vtable}. Every libio vtable is defined in a unique section called \textit{libio\_IO\_vtables}. By definition, libc trusts
the vtable if the vtable of the current FILE object lies within this section. It checks if the offsets of this vtable lies between 
\textit{\_\_stop\_\_\_libc\_IO\_vtables} and \textit{\_\_start\_\_\_libc\_IO\_vtables}.  If it does, we can continue with the operation; if not, libc 
conducts a final check by calling \textit{\_IO\_vtable\_check} which makes final checks on the FILE vtable pointer location, namespace and edge cases
where FILE pointer  objects are passed to threads, which are not in the currently linked executable.  When DCE overflows the \textit{\_IO\_FILE\_plus} and 
overwrites the \textit{\_IO\_jump\_t}, it does not lie in \textit{libio\_IO\_vtables} section and it also does not pass the pointer mangling sanity checks, leading to 
a \textit{Fatal error: glibc detected an invalid stdio handle} runtime error.

\subsection{PIE Loading and Usage}
Position-independent-executables (PIE) are applications compiled with special compiler flags that allow the application to be loaded into an arbitrary
memory address, not depending on absolute symbol addresses. DCE needs to have position independent executables, so that when they are loaded into memory,
the symbol positions in memory are dynamic, giving control over when the application main is called in a simulation.  To implement this, DCE uses the \textit{CoojaLoader}
which uses \textit{dlopen} under the hood to load the executable into memory. In glibc library versions newer than 2.25, security checks have been introduced to 
identify such PIE objects being loaded through \textit{dlmopen}, and in case the \texttt{DF\_1\_PIE} flag is found in the object's ELF dynamic headers, it will abort 
with an error.
 
\subsection{Customizing \emph{glibc}}
The solution to the above two issues is to provide a custom glibc for DCE.
Initially we looked for simpler solutions to disable security checks through compilation flags, but none were found. We also reached out to glibc maintainers for 
suggestions on alternatives or introduce patches to match our needs, but made no progress, so we decided to make a custom glibc based native build which came with some
installation cost as the build required increased disk space usage from about 8 GB to about 14 GB of space. This motivated us to also pursue a Docker alternative with a simpler setup and disk usage similar to older releases.

The DCE script \texttt{dcemakeversion.c} helps generate .version files for all the system shared library components that are statically linked to DCE's custom libc. A 
shared library \texttt{libc-ns3.so} is created with symbols tagged as either NATIVE or DCE based on their position of declaration, which are mapped through the 
\texttt{libc\_setup} function. The DCE type symbols are implemented 
inside the DCE source code. Users compile their applications with an extra -fPIC and -pie flag, which helps DCE load the applications dynamically into the DCE process 
address space. The \emph{main} function of these application are then called based on the simulation script, which if requiring FILE operations would require DCE to override the vtable 
mangling security checks of glibc, 
for which it is necessary to override the security checks on vtable pointer mangling. A patched glibc (with security checks bypassed), is built and installed on a specific location. and gcc is then made 
to use that directory as the SYSROOT and also point the library, header and ELF executable lookup directories to our custom glibc using -L, -I, --rpath and --rpath-link.

\section{NETWORK STACK ISSUES}

With a bit more porting effort, previous versions of DCE managed to support
the Linux (and FreeBSD) networking stacks as alternatives to the native ns-3
TCP/IP stacks.  However, the fast-paced Linux kernel evolution creates a
maintenance challenge, and the most recent Linux kernel supported by DCE
has been Linux kernel 4.4 (from 2016).  To upgrade this, we faced a choice 
of either staying with the current solution, \emph{net-next-nuse}, within
the LibOS framework or moving to a newer project
called the Linux Kernel Library (LKL).
Both LibOS and LKL have been compared on various parameters below.

\subsection{Linux Kernel Library}
LKL is a thorough library port of the Linux kernel that, through some pre-shipped helper shell scripts, can be used to hijack
all system calls made by a host application and map them through the ported Linux kernel rather than through system defined implementations. It also allows one 
to setup network interfaces such as TAP, RAW, VDE , etc. with custom gateways, netmasks, IP addresses, etc. with the help of JSON configuration files. These helper
shell scripts make use of \texttt{LD\_PRELOAD} to reorder library loading to LKL written system calls to take control in place of the libc defined routines.
 
\subsection{LibOS}
LibOS uses the same internal architecture as does \textit{net-next-nuse},
the Linux DCE kernel framework for previous releases.
It is also a Linux kernel port that works on the principle of selective 
kernel module linking and patching. It defines special link time constructs
to include only specific kernel files and 
symbols that are needed for executing on top of the Linux kernel with \textit{nuse}, which works similar to LKL by hijacking system calls 
and rerouting them through \emph{net-next-nuse} and kernel-defined routines.

\subsection{LKL vs. LibOS}
We conducted a comparison of LKL and LibOS approaches according to different design parameters and considerations for a complex application framework such as
DCE with very specific demands from its underlying network stack.

\subsubsection{Linux Feature Support and Maintenance}
Regarding the ease of extending support for Linux features that are compiled into the shared library port and 
can be used through the API, LKL seems to win here, as adding support for new features in later kernels can be as easy as a git rebase, because no kernel internal code  
is manipulated and also because the kernel is built using minimal changes to the original Linux build steps. LibOS, on the other hand, would require selective Linux 
kernel linking, bridges the gap between application workspace and Linux kernel networking stack with the help of glue code, kernel component connector code, and user 
application provided exported functions and callbacks for proper execution, which increases maintenance cost.

\subsubsection{Library Kernel Boot Order and Scheduling}
Both LKL and LibOS differ from each other on how they initialize the kernel and schedule tasks, which also determines many execution factors of DCE. 

LKL spins up a high level CPU lock-controlled thread, which makes calls to \texttt{start\_kernel}. Since LKL is a uniprocessor system, it initializes 
the kernel on a singular LKL thread, 
locks of which are synchronized with Linux scheduler calls, which are called when the scheduler decides to switch execution control 
to kernel level tasks for preemption, and other tasks, which require certain memory level moderation to achieve
atomic operation. Since the LKL thread can only be preempted by kernel level tasks and not through the LKL API, launching a second user task is not possible if 
CPU lock is held, blocking the DCE script execution, which works based on LWPs, infinitely.

LibOS makes 
use of its own \texttt{lib\_init} function which calls specific setups calls required to initialize the kernel.
It also overrides scheduler member functions, syncing them with ns-3's 
scheduler. LibOS creates an ns-3 task copy for each kernel task which is created by the \texttt{copy\_process}, \texttt{create\_process},
etc. Each such task maps with itself a callback function which should be called once the wait time
for a task is over, or if invoked as a part of a regular scheduling process. Tasks are processed by popping the ns-3 task queue.

\subsection{\emph{net-next-nuse-5.10}}
\emph{net-next-nuse}, a partial architecture port of the Linux kernel based on LibOS, was chosen over LKL due to design restrictions imposed by LKL's 
complete architecture level port of
the Linux kernel with uniprocessor limitations and scheduler independence. 
Upgrading net-next-nuse presented additional challenges, as discussed below.

In this section, we review some of the major challenges which net-next-nuse solves and the basics of how the Linux kernel is modified to 
accommodate this type of architecture.

Net-next-nuse understands that the way Linux kernel behaves on the host, and the way the kernel interacts with the task 
scheduler and how the scheduler operates on the tasks, is completely different from how DCE manages fibers (called tasks in DCE terminology). 
DCE is based on context-based, lightweight threads called fibers, which require a custom scheduler, called the TaskManager in DCE. On an ordinary
host machine, when a user runs any application that spins up normal threads, it makes the kernel responsible for jumping from one thread to another. 

Net-next-nuse uses fibres, which lets one jump from one thread to another only when one of the fibers yields to another fiber, and this context switch 
is done by a custom scheduler, which is written in DCE's TaskManager::Schedule(). This is beneficial because when using ordinary threads, 
the number of context switches the CPU has to do
results in more overhead than making switches in fibers. This also eliminates the dependence of the kernel to schedule the threads on 
the different cores of the host CPU and to maintain mutex locks of the CPU inside the kernel library. To manage the isolated kernel threads inside the library, 
net-next-nuse  performs a selective linking of 
kernel modules and utilities. To avoid linking the scheduler, workqueue, waitqueue and other such linked components, net-next-nuse instead implements
them using DCE imported functions mapped to TaskManager utilities. As a result, every time that some process wants to preempt and block, the kernel calls 
\texttt{schedule()} or \texttt{schedule\_timeout()}, and control is passed to DCE to take care of it. 

\subsubsection{Virtual Kernel Task and its Real Fiber Equivalent}
net-next-nuse uses exported functions and internal DCE calls to allow kernel task creation,  which is exposed to net-next-nuse through an API during library object initialization.

\sloppy Every new task inside the kernel-- for example, a syscall--
creates an internal kernel thread.  When a system boots up, there has to be some initial task that invokes the \texttt{start\_kernel(...)} 
function. This is called the \texttt{init\_task}. Whenever a new kernel thread is to be created, a call to \texttt{kernel\_thread(...)} is made. Along with the 
function that is to be invoked in the thread, and the arguments to be passed, one must also pass along one more argument, known as clone flags. 
Internally, the kernel will try to clone a previous task as much as possible. These flags basically determine 
how far the kernel should clone the structures. Some of the flags are \texttt{CLONE\_FS}, \texttt{CLONE\_VM}, \texttt{CLONE\_FILES}, etc., 
and \texttt{kernel\_clone(...)} passes a special clone flags data structure. This function further calls \texttt{copy\_process(...)}, which is responsible for checking 
which flags are enabled using a bitwise \texttt{\&} operator and calling the corresponding copy utility-- for example, \texttt{copy\_fs(...)}, \texttt{copy\_files(...)}, 
and \texttt{copy\_cred(...)}.  It also makes a call to \texttt{dup\_task\_struct(...)}, which will bind changes of the current \texttt{task\_struct} to a new one and 
return it back. After receiving back a proper \texttt{task\_struct}, the \texttt{kernel\_clone(...)} schedules a fork for the newly created task.

\sloppy Therefore, to take  control over the kernel thread creation, we rewrite the \texttt{kernel\_thread(...)} function to call 
\texttt{lib\_task\_start(...)}, which calls the 
internal TaskManager::Start(...) to create a DCE task, fills up the task\_struct, sets the SimTask context with the task\_struct and returns back 
the process ID (pid) of the task. Also, \texttt{current}, as previously referred to above, is not a variable, but a macro, which calls \texttt{get\_current(...)}, which DCE hijacks and leads to TaskManager::TaskCurrent(...), which checks if the current task has a SimTask context. If it does, it returns it back. 
If it does not, it calls TaskManager::Start(...) to set it up, and every time 
TaskManager::TaskWait(...)  is called on a particular task, DCE can put the current job to sleep, and give the other tasks/fibers a chance to 
execute their set of functions. 
 
\subsubsection{Kernel Initialization}
The kernel initializers are divided into base init functions and roughly eight levels/types of initcalls, and export a start and end pointer of the list of initcall list. 
The initcalls are stored in a special .initcall.init section of the final linked library, which are then copied to their respective positions through the linker.lds script.
Some of the new additions made to original initcall list are: 

\begin{itemize}
  \item vfs\_caches\_init\_early: To allow mounting of file systems such as \texttt{/proc} and file operations, which require the linked lists for 
  dcache (Directory Entry Cache, which helps fast lookup of recent paths) and inode.
  \item cred\_init: Steps up the \texttt{creds\_jar} cache which is required by the LSM (Linux Security Module) \hyperref[Section_LSM]{3.4.7}, fs module and 
  while creds initialization for kernel task creation.
  \item proc\_root\_init: Sets up the \texttt{/proc} filesystem root cache, symlinks for mounts, net namespace for proc, and required directories.
\end{itemize}

The initcall invocation loop of lib\_init was also changed to use \texttt{initcall\_from\_entry} instead of a direct function call. 
This invocation function does a safe offset based function address lookup and calls the init method accordingly. 
Before we call all the init functions, we also initialize the \texttt{files} and \texttt{fs} cache pointers for cache based memory 
allocation using \texttt{kmem} in file operation methods, with slab objects aligned to a cache line, panic enabled on relocation errors, and also accounting enabled 
for the cache. These caches are required while setting up the \texttt{fs\_struct} and \texttt{files\_struct} for each new task in its 
corresponding \texttt{task\_struct}. 

\subsubsection{Security LSM Module}\label{Section_LSM}
Certain kernel initialization procedures proceed only when the desired actions are allowed by the LSM. Below is a brief description of why the LSM is required and how 
it is included into net-next-nuse.

In newer Linux releases \texttt{vfs\_kern\_mount(...)} validates the file system mount context and parses params using \texttt{security\_fs\_context\_parse\_param(...)},
which checks if the requested operation was blocked in the LSM tables. If this module is not enabled, it would return back ENOPARAM by default and fail. This makes it 
important for us to link the LSM, and setup our build/linking scripts accordingly.

The security module of the Linux kernel depends on a special configuration called the \texttt{CONFIG\_LSM}. The LSM module requires a few architecture
defined variables, which are placed in a specific section
of the library called .init.data section using the \texttt{AT(ADDR(...))} directive, through linker script. 
We keep sections \texttt{lsm\_info.init} and \texttt{early\_lsm\_info.init} even if not referenced using the \texttt{KEEP} directive, 
and also define start and end information sections of both.  The implementation of them were drawn from LKL's LSM definition.

\subsubsection{Kernel Code and \emph{net-next-nuse} Alignment}
When net-next-nuse is loaded as a shared library into DCE and \texttt{lib\_init(...)} is called, it invokes certain kernel methods, which triggers 
kernel initialization routines which help setup critical components such as the mount namespaces, vfs, kernel threads and tasks, etc. 
Since Linux kernel 4.4.0, the
kernel has evolved a lot in terms of how the kernel gets initialized. The section below discusses some of the alignments which had to be made with respect to 
Linux kernel 5.10 to integrate net-next-nuse's architecture.

The kernel depends on certain namespace based constant variables while initializing the kernel. One such requirement is the \texttt{UCOUNT\_MNT\_NAMESPACES}
of the \texttt{init\_task}. Since a complete architecture port is not needed, this value is not defined, so atomic reads on this value would fail. 
We manually set it to \texttt{MNS\_MAX} for the current task's nsproxy. One other alignment was the \texttt{sock\_init}, which is dependent on the proc file system  
to setup the sysctl interface using \texttt{net\_sysctl\_init(...)}, and then the sockfs filesystem is registered using \texttt{register\_filesystem(...)}. Previously,
sockfs file system init did not depend on a file system context and required only a mount function \texttt{sockfs\_mount}, but in a later Linux commit the sockfs 
mounting process was handed over to the internal VFS mounting initcall, thus we had to glue code the \texttt{mnt\_init(...)} initcall. Many other alignments, such as,
atomic operations, NAPI atomic read issues, etc. were fixed based on later Linux commits.

\iffalse
\subsubsection{Native Kernel NetDevices}
Certain components of ns-3 require the setup of custom NetDevices which require a custom MTU and other flags, such as whether the device should enable multicast, 
or is it a point-to-point device, etc. These flags can be put together, and one can allocate a Linux netdevice struct for each such requirement using
the \texttt{alloc\_netdev(...)}, passing all of the configuration needed for details such as MTU, address length, and destructors, along with 
a \texttt{register\_netdev(...)} call.
This returns back a forward declared struct SimDevice which is used as a NetDevice object inside ns-3 which is required by ns-3's core networking modules.
\fi

\section{BUILD AND INSTALLATION ISSUES}

\subsection{Bake Build Automation}
The process of the modified glibc and Linux compilation can be automated using Bake.
Bake has a source configuration option named \textit{patch}, which can safely apply a patch.
Using this option, Bake applies the DCE-specific glibc patch, which disables the security checks on vtable mangling, and also disables the 
PIE object checks for dlmopen position independent executable loading. The glibc is then built using its standard build steps. The Linux kernel headers
files are then installed into the \texttt{/usr} directory of a custom glibc's sysroot. This finishes with a standard Linux-like system root to use for building 
DCE without any issues. 

\subsection{Docker Environment for DCE}
DCE has a long dependency graph that includes dependencies across 
system libraries, other projects, Linux kernel and tools, host specific requirements, etc. To ease the installation process and to standardize the process 
across Docker supported Linux distros, a Docker based setup is discussed.

\begin{figure}[!htb]
  \centering
    \includegraphics[width=0.30\textwidth]{figs/docker-architecture.jpg}

  \caption{DCE Docker Architecture Composition}
  \label{fig:docker}
\end{figure}

The Docker release consists of the Docker network setup shown in Figure \ref{fig:docker}, which is orchestrated using a Docker Compose script. 
It consists of the Docker container, environment variables and volume. The Docker 
container works on a custom image using Ubuntu-20.04 as the base image. A Docker image is collection of image layers. Each image layer is a 
Dockerfile command which adds up to create the final Docker environment for that image. The Docker image has been optimized to use a minimal number of layers, and 
all redundant layers are merged together. A layer for installing all system dependencies and libraries ensures all listed and unlisted bake 
dependencies for building DCE are satisfied. It also has a pre-compiled glibc bundled into the image itself. This helps us reduce the Docker 
image size drastically as compared to building from source every time, which might consume GBs, but this layer takes up roughly 50 MB.
It also has a default environment variable 
\texttt{DCE\_WITH\_DOCKER=1} which is a flag variable for DCE's Waf script to try and execute a Docker based build.

The container, as discussed before, comes with a pre-compiled glibc-2.31 and so the user does not need to build the modified glibc again from source.
The environment variable \texttt{DCE\_WITH\_GLIBC} tells Waf where to look for the modified glibc build directory.  This environment variable can 
later be changed from inside the Docker container to point to a custom glibc build directory if the user wishes to do so. This glibc build is then 
used by the DCE Waf to compile DCE. The Docker compose also sets up a volume. A Docker volume is a mechanism which lets Docker containers 
use persisting data from within the container. Docker containers can attach volumes to themselves
and map specific directories from withing the Docker container to the host directory. Any changes made to that directory within the Docker container
are visible and persist on the mapped host directory, and also vice-versa. This is a key requirement for us, as ns-3 projects are used by both 
end users who wish to just simulate programs without library changes, and researchers who need to be able to make changes 
to the ns-3 library code and rapidly test it, and also access simulation output files on local file explorers and 
third party applications. We  therefore attach a volume which maps the \emph{/home/bake} directory from within the Docker container to the bake directory on 
the host.

The DCE Waf script supports both Docker based build and native builds on the same branch. It selects the glibc 
build directory based on Waf configure and local environment variables and compiles DCE on top of it.

\section{PERFORMANCE EVALUATION}

We describe some performance evaluations of the DCE code, following the
approach described in \cite{Tazaki13}.  The purpose of this section is to
compare the runtime performance of different versions of DCE and different
DCE configurations with the performance reported in \cite{Tazaki13} and with
a equivalent ns-3 (without DCE) simulation.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.43\textwidth]{figs/topology.png}
  \caption{Simulation Topology (Redrawn from Figure 2 of \cite{Tazaki13})}
  \label{fig:topology}
\end{figure}

The simulation topology used is shown in Figure \ref{fig:topology}, which
is duplicated from Figure 2 of \cite{Tazaki13}.  The topology is a simple
linear network with a variable number of hops, and a single UDP flow
is sent from client to server for the duration of the simulation.
The UDP data rate is configurable between 5 and 100 Mbps, and the UDP
payload size was a uniform 1470 bytes per packet.
To avoid per-hop congestion, link bandwidths are set to 1 Gbps.
For the purpose of runtime performance comparison, this topology is
suitable because it makes use of a DCE application (a custom UDP traffic
generator (\emph{udp-perf}) written in C and maintained as part of DCE),
and allows the enabling or disabling of kernel-mode operation.  Most
simulation operations involve the creation and transmission of packets.
The program to realize this topology is found as an example named
\emph{dce-udp-perf.cc} in the DCE codebase; we only had to make minor
updates to that program likely used by \cite{Tazaki13} to realize the
different configurations reported herein.

We do not know what version of DCE was used in \cite{Tazaki13} but we
have reason to believe that it was not significantly different from the
most recent DCE 1.11 release of DCE (for Ubuntu 16.04), because there
has not been much development of DCE since the publication of \cite{Tazaki13}.
The new software we are reporting on herein (for Ubuntu 20.04, with newer
kernel code, and optionally with Docker) will be part of DCE 1.12 and
later releases.

ns-3 and DCE software can be compiled in debug or optimized mode.  For
these experiments, we compiled both ns-3 and DCE in optimized mode.  All
other software (e.g. net-next-nuse-4.4.0, glibc-2.31) was compiled in
the default mode specified for that software.

For the experiments, we used two different machines (for the two different
operating systems).  As DCE execution time is dominated by the single thread
CPU performance of a machine, execution of the same program on different CPUs
will lead to
different runtimes.  In \cite{Tazaki13}, an Intel Xeon 2.8 GHz machine was
used; this CPU is rated with a single thread performance of 384 by PassMark
Software's online CPU benchmark dataset.\footnote{https://www.cpubenchmark.net}
We used an Intel Core i7-4770 3.40 GHz machine for tests of DCE version 1.11
on Ubuntu 16.04.  This CPU has a single thread benchmark of 2167 according
to the same database.  For tests of DCE version 1.12 (Ubuntu 20.04), we used
a machine with an Intel Core i7-1065G7 1.30 GHz CPU.  This CPU has a
single thread benchmark of 2416 according to the same database.  Use of
different machines for the two different operating systems makes it difficult
to directly compare the DCE performance results, but because we are able to
run the same ns-3 simulation on both machines, we can calibrate the
performance by referencing DCE performance results in relation to the
corresponding ns-3 runtimes on each machine.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.43\textwidth]{figs/hops-vs-pps.png}
  \caption{Packet Processing Performance as a Function of Number of Network Hops}
  \label{fig:hops-vs-pps}
\end{figure}

Figure \ref{fig:hops-vs-pps} reproduces data from a similar figure (Figure 3)
in \cite{Tazaki13}, and provides some new data for current simulation runs.
In \cite{Tazaki13}, this figure was used to compare performance between
DCE and Mininet-HiFi.  Here, we compare the previous data on DCE from
\cite{Tazaki13} (read from the paper figure) with a similar experiment run
with DCE version 1.11 on Ubuntu 16.04.  A single simulation trial for different
values of network hops (4, 8, 16, 24, 48, 64) was run for 50 simulated
seconds, and the number of received packets was divided by the elapsed
wall clock time of each trial, to yield a packets-per-second measurement.
This experiment illustrates that the most recent release of DCE code has a
similar performance profile as that of \cite{Tazaki13}.  Because the
number of packets sent in each simulation is the same, the downward
slope of the curves as the number of hops increases indicates that larger
topologies (which cause more simulation events to be generated per packet)
will run more slowly.  We also note that an unexpected result of this
experiment was that the two curves would generally overlap, since the
simulations were conducted on machines with different CPU performance.
This could indicate a performance regression between the version of DCE
used in \cite{Tazaki13} and the most recent release; performance regression
has not been tracked as part of DCE maintenance.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.43\textwidth]{figs/rate-vs-time.png}
  \caption{Sending Rate vs. Wall Clock Time on a Four Hop Topology for Different DCE Configurations (Ubuntu 16.04)}
  \label{fig:rate-vs-time}
\end{figure}

Figure \ref{fig:rate-vs-time} recreates a similar experiment 
(shown in Figure 5 of \cite{Tazaki13}) to highlight the impact of sending
rate on the wall clock time of the simulation trial, for different sending
rates on a four-hop topology.  As in \cite{Tazaki13}, a linear relationship
between the sending rate and wall clock time is expected, because the
number of hops per packet is held constant.  Again, we reproduce the
curve from \cite{Tazaki13} by reading their data from the paper figure.
There are several interesting observations.  First, we believe that the
closest comparison between the experiment from \cite{Tazaki13} is the
\emph{DCE 1.11 ELF} result.  This configuration used DCE with Linux kernel
version 4.4.0 and the ELF loader (known to lead to faster simulation
runs than the default Cooja loader).  Despite executing on a much faster
machine, the DCE 1.11 result is slower than the earlier reported simulation.
Again, we do not have specific information on how the previous run
was conducted (the website hosting the scripts is no longer active), and
it could possibly be due to performance regressions either with DCE
or with the Linux kernel library (\cite{Tazaki13} used a virtualized kernel
version of 2.6.36).  Aside from this difference, this figure can be used
to compare the execution time performance of different modes of DCE.
For comparison, we added a mode of operation to \emph{dce-udp-perf.cc} to
bypass the use of the \emph{udp-perf} DCE application and to use the
ns-3 UDP client application.  The blue line (bottom) is therefore a pure
ns-3 simulation, and runs fastest.  The next fastest execution is found
with \emph{DCE 1.11 ELF user}, which is using the ELF loader with DCE
running in user-mode, not kernel-mode.  This means that the DCE application
is using the ns-3 TCP/IP stack rather than the Linux kernel stack.  The
next fastest configuration is the \emph{DCE 1.11 ELF} configuration, using
DCE user-space and kernel code over the ns-3 simulated point-to-point links.
The final curve is the \emph{DCE 1.11} curve that uses the DCE user-space
and kernel code but with the default Cooja loader.  This curve shows that
the use of the ELF loader leads to a significant performance benefit.
While Figure 5 of \cite{Tazaki13} includes plots for other network
diameters (up to 32 hops), we omit the larger topologies in our figure
for clarity; the larger topologies display similar linear relationships.
The horizontal dashed line depicted shows the performance point at which
the simulation time equals the wall clock time; only the DCE 1.11 curve
for data rates greater than 60 Mbps ran more slowly than real time in
these experiments.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.43\textwidth]{figs/rate-vs-time-dce12.png}
  \caption{Sending Rate vs. Wall Clock Time on a Four Hop Topology for Different DCE Configurations (Ubuntu 20.04)}
  \label{fig:rate-vs-time-u20}
\end{figure}

Figure \ref{fig:rate-vs-time-u20} presents comparable simulation results
for the new version of DCE (the pending 1.12 and later releases).  As noted
above, the machine for these experiments had a faster CPU, and in comparing
the pure ns-3 results between both machines, the simulation runs on this
machine were about 25\% faster (although the ns-3 versions were different,
ns-3.34 on DCE 1.11 and ns-3.35 on DCE 1.12, this is not expected to
lead to any notable difference in performance).  The next curve above
the ns-3 curve, \emph{DCE-1.12 user}, can be compared with the
\emph{DCE-1.11 ELF user} in the previous figure.  In Ubuntu 20.04, the
ELF loader is not yet working, so the comparison is not exact, but the
experiment shows that despite the lack of ELF loader, use of the ns-3
stack in both cases leads to faster than real-time execution.

Next, compare the red curves, \emph{DCE-1.11} vs. \emph{DCE-1.12 kernel 4.4}.
Neither configuration used the ELF loader, and both used the same Linux
kernel library version (4.4).  However, the DCE-1.12 version uses the
modified glibc library.  The newer DCE version results in 75\% more runtime
despite being on the faster machine.  

The final two curves show performance differences between the current
kernel library version 4.4 and the newer library (5.10), and the
difference between running DCE natively on the host operating system vs.
running within a Docker container.  The newer kernel library required
about 16\% more runtime.   Running within a Docker container incurred
only about a 3\% performance drop.

As of this writing, the performance differences between the newer DCE 1.12
versions and the DCE 1.11 release have not been debugged
or profiled; the differences were discovered during the course of preparing
this paper.  The small performance cost of running within a Docker container,
however, may be worth the convenience to future users.

\section{RELATED WORK}
We do not provide a lengthy list of other simulation frameworks related
to DCE, or other different approaches to mixing implementation code with
simulations; the interested reader is directed to \cite{Tazaki13} and
\cite{Lacage10} for such references.  We focus here on related work that has
been published since \cite{Tazaki13} was published in 2013.

We are not aware of anything comparable to DCE having been published in
the past eight years, either for ns-3 or for other system simulators.  
During that time period, more attention has been paid on the use of
ns-3 emulation features or on container-based network experimentation
frameworks such as Mininet \cite{Handigol12}; a recent tool in this regard is
NeST \cite{Rai20}.  One of the most similar approaches to DCE appears to
be the Open Network Emulator (ONE) \cite{Duggirala12}, which leverages the
Clang/LLVM
toolchain to convert applications to modules that can be integrated
with a network simulation stack.  The NCTUns simulator \cite{Wang03}
is a commercial simulator that advertises a 'kernel-reentrant' mode
of supporting Linux network stack code in a simulation.  Both of these tools
were developed during the same timeframe as DCE.

There also appears to be very little published regarding the extension
of DCE or analyses of its performance.  The work by Ivey \cite{Ivey16}
describes an extension of DCE to support not only C and C++ but also
Python- and Java-based programs.  The main interest in doing so was
to enable DCE use of software-defined networking (SDN) applications,
many of which are implemented in Python or Java.  However, these
extensions appear to never have been publicly released.  
One paper published about five years ago \cite{Wiggins16} began to explore
a possible performance bottleneck arising from how DCE makes use of multiple
cores to distribute processes onto different threads, and speculates that
cache consistency overhead may account for the observed counter-intuitive
result that a DCE experiment may run significantly faster on a single core
machine than on a multi-core machine.   Further profiling of our performance
results in this regard is definitely warranted. 

\section{CONCLUSIONS}
%\end{document}  % This is where a 'short' article might terminate
The paper discusses the way DCE was modernized to work on newer Linux environments, 
use newer Linux networking stack and kernel and also compares against other alternatives. 
It also suggests ways to improve project deployment using Docker and compares performance results across different platforms and stacks.

Some planned next steps include profiling DCE further to understand 
parameters determining the runtime of programs. We also have thoughts about
building a more modular Linux library port with an efficient 
middleware notification based task synchronizer, which would have less maintenance cost and increased kernel feature availability.
 The port of net-next-nuse to Linux 5.10 requires completion of IPv6 support.
Restoring elf-loader compatibility for libc-2.31 is also a priority, as it
significantly reduces DCE execution time.  Finally, we must update to the
new ns-3 CMake build system.

%ACKNOWLEDGMENTS are optional
%\section{Acknowledgments}
%This project was funded by the 2021 Google Summer of Code.
%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{ACM-Reference-Format}
\bibliography{revitalizing-dce}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
%\balancecolumns % GM June 2007
% That's all folks!
\end{document}
